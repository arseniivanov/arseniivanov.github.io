# Why the MoE pattern is inevitably a part of the future of compute in ML

### What is MoE?

MoE, or Mixture of Experts, is an architectural block used in several open-source large language models where the input matrix is projected using a router to select one or more out of N expert, which gives you N "experts" that are most suited to handle the input. The input data is then piped to the experts who are selected, and is usually transformed using an MLP. The output is then aggregated somehow, sometimes using weights for each expert, sometimes linearly, sometimes summed and normalized, and passed onto the next architectural block. 
![[Pasted image 20250917200745.png]]

This is different from the Transformer block at this stage, which just has a massive MLP that handles every task by itself. Here is an image taken from [here](https://blog.dailydoseofds.com/p/transformer-vs-mixture-of-experts) that shows the subtle difference between the two architetural blocks.
![[Pasted image 20250917201210.png]]

### Why MoE?

Compared to the transformer block in the image above, the MoE block looks just more complex, less expressive, and at first glance seems like an optimization made to cut inference costs.

#### Near-optimal use of existing compute medium

However, the MoE structure is incredibly well-suited for GPUs both on a single- and distributed multi-GPU scale. The operation is a simple small GEMM, which is compute-bound on GPUs, which means that its as efficient as it can get, the on-GPU memory transfer is not the bottleneck as with many other architectures. The small expert size means that its you can route less data to other GPUs that hold their own experts. The modular nature of the architecture also means that it could be trained on more, different kinds of GPUs, and on other compute mediums such as CPUs which monolithic architectures cannot handle with their memory requirements.
#### Better use of power, and better margins for companies

Let's say you are tired and want to not think that hard about something, you run autopilot to select something to eat or watch completely at random, this would use less agents in your brain, or you would let one agent have overwhelming signal(similar to System 1 thinking from Kahnemans theories). The same way, MoE systems can dynamically decide on the amount of agents/blocks to include in a decision based on the question complexity profile, or in the future perhaps even based on the available energy at this time using hard throttles or router projections. A simple question like "What is the capital of France?", might not need to use the energy equivalent of a query that goes through 20'000 pages of text and summarizes the content. A question when energy prices are higher might not use as many agents for a user on a subscription-based model.
#### Modular design

There has also been research on modular MoE architectures from a bunch of top US universities(https://arxiv.org/abs/2507.07024), where you can independently train different MoE models on **different** datasets, and using a frozen shared MoE block, you are able to combine and route related datasets efficiently. You can essentially stitch together independent MoE blocks trained on different datasets to a single model that by training your router to choose between either the frozen block of your block. This idea is really powerful when you think about what data means to a company. A medical company with sensitive data could train a MoE block locally, and then send it and attach it to a larger, smarter medical model is able to harness the information that was previously unavailable.

This unlocks gating and selection on a different abstractional level. A model can for example start monitoring which of it's blocks provide value to the queries that it receives. It can start looking at projections of the planes, seeing when a new block is redundant, or when two blocks could be fused with each other based on information-theoretical metrics.
#### Arguably similar to theories on human thought and decision making

The logic of MoE draws connections to Marvin Minsky's [Society of Mind](https://en.wikipedia.org/wiki/Society_of_Mind) that posits the theory that in the human brain, there are a lot of  "agents"/interconnected neuron groups that get activated by various external (bottom-up), and internal (top-down) input, and the output/your thoughts and decisions is the reduction over the highest signal components of this massive chaotic mess.

### What I think the future holds for MoE

I expect that given the hardware efficiency, the modularity, and the energy/cost optimization opportunities, MoE is solidified for the future.

I believe that the future holds:

**More advanced routing**. Currently, the router is a simple projection that builds upon the premise that all data that arrives to it is on the expected abstraction level. This might not be the case in the future, especially if you receive data from different sources. A routing block would need to understand what kind of level of abstraction the query is at. You could also as mentioned add energy controls here. 
(*i also believe that initial data encoding/embeddings are overlooked today as an informational bottleneck, but that is for a different blog*).

**Cross-expert context**. During or after transforming an input, a block could immediately notice that it could be valuable for the question at hand to include another expert in the decision at the same level of abstraction. Either the router would have to be informed, or there could be cross-expert contexts shared during inference time to improve the outputs of the expert blocks.
Either way, this would allow some "test-time-compute"-iterations at various levels of concept abstraction.

**Expert "signatures"**. Extending on the previous point, each expert could hold a projection of it's learned concept in a shared embedding space in order to improve the search & retrieval amongst the experts once the space grows.

**Self-pruning/Concept fusion**. When a shared embedding space such as above is established by providing the MoE experts with signatures, it would be possible to let the model continuously evaluate itself. Are some experts never used? Are two experts always used together? Is a new expert actually useful given the existing knowledge? This would allow a model to build and optimize its knowledge landscape dynamically. 

---

In general, I believe that a lot of the "test-time compute" will be done on the level of MoE/MLPs directly, instead of forcing the model to re-ingest its output in a CoT context.  Similar work has been done for the ARC-AGI competition in 2024, where LPN(https://arxiv.org/pdf/2411.08706) was used to gradient-descent the internal embedding space of the model in order to find the best solution at test-time. I believe that this search could be done in the embedding space of MoE projections, both internally for the experts, and externally in the space of expert projections in order to select experts to include in the decision.
