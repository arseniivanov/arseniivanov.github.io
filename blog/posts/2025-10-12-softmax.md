# What is attention and why do we need Softmax?

FlashAttention 4(FA4) recently got [unofficially released](https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py) which seems to be the first non-cudnn Petaflop-scale attention kernel clocking in 1 620 000 000 000 000 floating point operations per second.
Some people at Modal have done a fantastic breakdown on this topic in [their blog here](https://modal.com/blog/reverse-engineer-flash-attention-4) which will be the basis for the discussion here.

This short blog is mainly a thought after reading the blog.
## Back to Async comms

The Blackwell architecture boasts new Tensor Core-specific memory, which adds another memory architecture on top of Shared memory, which means more complex memory comms. I dont intend to break down the HW arch in this blog, but if you are interested I recommend looking terms up in the [GPU Glossary](https://modal.com/gpu-glossary/device-hardware) as you read along.
Looking at the code, the amount of state-machine-like thread specializations [in the source code](https://github.com/Dao-AILab/flash-attention/blob/5c1627a7a1cda9c32cb9b937a053564e663f81bc/flash_attn/cute/flash_fwd_sm100.py) that make things look like SHMEM-tiles are the new primitive to think about as units in a state machine flow.

Every kernel seems to pass around memory barriers and some threads are responsible for updating and making sure that communication is only made when necessary.
This creates a massive complex communication web inside of every tile and between the tiles which the programmer has to manage.

Looking in detail at the code, it would be incredibly hard to outsource this logic to the compiler. After asking experts in the field, they agree that the timeplan for compiler optimizations are 1 year+ for this. Comparisons to GCC are drawn, however, there is an agreement on that both the C standard and the x86 architecture were pre-agile era monoliths that were frozen after release, which made compiler optimizations easier. Meanwhile, NVIDIA hardware has a 2 year release pace, and the software gets new features and releases on a monthly pace, making any compiler optimizations much, much harder to make.

It seems like the future kernel developer will have to understand, and think about async comms on a massively parallel scale.
Charles Frye mentions this at [this point of his presentation](https://youtu.be/VPslgC9piIw?si=PDQimAGiTifWf6Ux&t=3437), where we essentially have an async model on a warp-level SM granularity running in synchronous lockstep. So we use each SM as a little conveyor-belt that fully controls its own dataflow and inter-communication, this is an increase in complexity and requires a new way to think. You now no longer only have to orchestrate the data movement saturation between HBM and SRAM but also the intra-warp movement and comms which is SRAM/register only.
## Softmax obsession ðŸ’‹

There is a massive focus on Softmax in each warpgroup for FA4, look:
```python
        self.softmax0_warp_ids = (0, 1, 2, 3)
        self.softmax1_warp_ids = (4, 5, 6, 7)
        self.correction_warp_ids = (8, 9, 10, 11)
        self.mma_warp_id = 12
        self.load_warp_id = 13
        self.epilogue_warp_ids = (14,)
        self.empty_warp_ids = (15,)
```

We have 12 out of 15 warps working on either performing softmax or normalizing values for softmax(correction warps), this is bizarre. A single warp is loading data and a single one is doing actual matrix multiplications. The reason for this is a smart outsourcing of software-emulated exponentiation when the Special Function Unit(SFU) is overloaded with exp-calculations.

This makes me ask the question, why is Softmax necessary? The Softmax in attention was introduced in the original Transformer paper as a way to normalize the QK results for better computation. However, we are already dividing the QK with a normalization value AND subtracting away the max value in order to not get exploding exponents. While we get a better optimization landscape, at some point we should ask ourselves if this is necessary because its clearly clogging up the whole pipeline. 

## Are the options to Softmax?

Of course, there is a whole family of models over at [flash linear attention](https://github.com/fla-org/flash-linear-attention) GitHub that I argue will be a part of the future of LLMs, because since [one of the years best papers from ByteDance](https://arxiv.org/pdf/2505.19488v1), based on Songlin's [Deltanet scaling paper](https://arxiv.org/pdf/2406.06484), they explore the family of attention's(memory association architectures) that are equvariant to other non-softmax representations. This shows that there is nothing magical about softmax, it improves the training by making a good optimization landscape, and it provides more specific informational association(less generality + less hallucination). 

[Qwen-3 Next](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list) has ended up adopting linear attention for 75% of its layers representations, which slashed training costs by a staggering 80% less GPU hours. And while not competing fully with the with closed-source labs performance, this is not necessarily because of the arch choice (a better comparison would be to LLAMA that uses transformers), but more on the other major influences such as data and training methodology.

In general, the Deltaformer motivation from the ByteDance paper uses quite rigorous logical and linear-algebra reasons to show that Key-Value retrieval in an orthonormal basis are optimal but also scale linearly with the amount of pair we want to encode. It also highlights the specialization capabilities of softmax in this space, learning more specifics above generalizations of for example ReLU.

This points to a future where we probably want more linear/ReLU-like transforms early on, to utilize the superposition principle in the projected orthonormal basis and get good use of the kernel trick to get the most generalizeable informational representations, and then use exp/softmax to either route between latent blocks of this representation in an MoE fashion, or add them later on to specialize once we reach the end of the model where we need to be precise and hallucinate less.

---

If anyone is interested in the more objective future of this topic, you can check out the ICLR 2026 open-review submissions for this topic [here](https://docs.google.com/spreadsheets/d/1ooCMOwkUup_WRsHJuu5Dmjj29lB60YrZPdUP-02kq_g/edit?gid=1013819544#gid=1013819544)
