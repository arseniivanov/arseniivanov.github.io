# Making TriMul go BRRRR for GPGPU

The fantastic folks over at GPUMODE decided to host a competition to accelerate the execution of **Triangle Multiplicative Update**, which is an algorithmic block for some BioML models like the AlphaFold3 model. This block is partially responsible for a Nobel prize in chemistry 2024 which gives it a bit more clout.

The challenge is to get it to run as quickly as possible on the server GPU's A100, H100, B200 and MI300.

The full description of the challenge can be found [here](https://stormy-sailor-96a.notion.site/GPU-MODE-Mini-Competition-3-AlphaFold-s-Triangle-Multiplicative-Update-207221cc2ffa8034b3eddff1d898dc14).

I took this on challenge, and this blog is a log and guide on how to optimize execution for various server cards.

The TriMul algorithm can be seen here in the reference implementation:

![[Pasted image 20250816103354.png]]
# Baseline

All the numbers and optimizations in this blog will be based on these two run configurations, more tests are run with more diverse configs, but to keep this blog reasonable I will take two of them and use them as the base of this blog. All times are taken from executions on the H100 GPU for these two configurations:

  - {"seqlen": 256, "bs": 2, "dim": 128, "hiddendim": 128, "seed": 9371, "nomask": True, "distribution": "normal"}
  - {"seqlen": 768, "bs": 1, "dim": 128, "hiddendim": 128, "seed": 381, "nomask": True, "distribution": "cauchy"}

The benchmark does a few runs of the configuration and reports the fastest(‚ö°) and slowest(üêå) run of the bunch.

The reference kernel gives us:

‚ö° 3.05 ms üêå 3.14 ms       
‚ö° 13.9 ms üêå 14.7 ms
# Optimizations

When optimizing, there is always a time-efficiency trade-off present. You can spend your whole life chasing bits, bytes and microseconds, but there is in general a performance/time trade-off curve that looks something like this:

![[Pasted image 20250816104939.png]]

Often, PyTorch compile will get you 50% of the way there with minimal time/effort. So will start with that to make a good start.

---

### Chapter 1: PyTorch.compile

Pytorch.compile() works the best when you ONLY use pytorch functions, as odd list comprehensions and other esoteric non-torch Python can cause graph breaks in the TorchDynamo/TorchInductor frameworks that optimizes the execution graph, so the first task is to rewrite the code to be as monolithic PyTorch as possible.

The first optimization is just removing the unnecessary abstract classes that duplicate data and create overhead, and working directly on the tensors with PyTorch.compile added for the function. This gives us:

‚ö° 2.06 ms üêå 2.19 ms       
‚ö° 10.1 ms üêå 10.5 ms

A practically free 33% performance increase, nice.

The next step is to look at the types. All of the GPUs have fp16 accelerators, so it would be good to see if we can lower the precision and end up with the same results. We can see that we are getting 2 distributions, normal and cauchy for the input weights. Cauchy distributions are much larger, and cover a very large range. However, we also see that that we have a LayerNormalization as the first input transform. This means that after that, we will be in range 0->1 which is much more workable with fp16. We try to set ALL the weights to fp16 except for the first LayerNormalization weights and the input weights themselves. This gives us a small improvement while maintaining an output within the error bounds.

‚ö° 1608 ¬µs üêå 1716 ¬µs       
‚ö° 8.50 ms üêå 8.58 ms

#### Sidenote 1
I tried to branch the kernel depending on the distribution by looking at the abs().max() > 10 of the incoming array to decide if its a cauchy dist and use fp16 for normal distributions. While we were able to separate the two distributions, this did not yield a performance improvement. My theory is that since we do inverse square root for this calculation, which is an SFU(Special function unit) function, we are forced to use fp32/fp64 anyways due to the hardware, causing an unnecessary cast.

---
We then realize something big that yields the overall biggest improvement to the kernel.
```python
        left = self.left_proj(x.to(torch.float32))
        right = self.right_proj(x.to(torch.float32))

        mask = mask.unsqueeze(-1)
        left = left * mask
        right = right * mask

        left_gate = self.left_gate(x.to(torch.float32)).sigmoid()
        right_gate = self.right_gate(x.to(torch.float32)).sigmoid()
        out_gate = self.out_gate(x.to(torch.float32)).sigmoid()
```

All of the gates and inputs are using the **same input** X together with a fairly small weight matrix <128, 128> or in general size <d, h>  which can be found in the configs. We are loading 5 small matrices, and then multiply them with the **same X** separately. Instead, we realize that we can stack all of the projection matrices together, and make a single matrix multiplication. This is fantastic as channel-based multiplication is much faster on the GPU, since we we keep the input matrix x in memory.

```python
    w_concat = torch.cat([
        weights['left_proj.weight'],
        weights['right_proj.weight'],
        weights['left_gate.weight'],
        weights['right_gate.weight'],
        weights['out_gate.weight']
    ], dim=0).t().contiguous().to(torch.float16) 
    
    ...
	
	all_projections = torch.mm(x_norm, w_concat)

```

We benchmark this and get a massive jump in performance:

‚ö° 421 ¬µs üêå 636 ¬µs       
‚ö° 6.04 ms üêå 6.23 ms

We can see that with this little optimization (still fully in PyTorch with about 25 lines of code), we have increased throughput for the first example with 8x and the second example with 2.5x from the baseline.

---
Here, I think of the next steps, and realize that they will need to combine:

Tiling, not controllable in PyTorch.
Operator fusion, not controllable in PyTorch.
Swizzling and other cache occupancy tricks, not controllable in PyTorch.

---
### Chapter 2: Triton

When optimizing for different hardware, we want to stay in GPGPU terrority, creating the fastest kernel that can be translated to run on all the different hardware, for this, Triton is a good language choice.

I start off in my comfort space, writing the Triton kernels for the initial MM and the BMM/einsum.

With a small set of autotune configurations we get:

‚ö° 2.10 ms üêå 2.90 ms       
‚ö° 10.1 ms üêå 10.2 ms

We are back to the slow second pytorch.compiled-kernel speed.
What we notice is that we have a lot of kernel launches:

```python
norm_kernel<> #kernel launch
mm_kernel<> #kernel launch
left_raw, right_raw, lg_raw, rg_raw, og_raw = all_projections.chunk(5, dim=1)
mask_expanded = mask.expand(-1, -1, -1, h).reshape(-1, h)
left = left_raw * mask_expanded * torch.sigmoid(lg_raw) #kernel launches
right = right_raw * mask_expanded * torch.sigmoid(rg_raw) #kernel launches
out_gate = torch.sigmoid(og_raw) #kernel launch
bmm_kernel<> # kernel launch
norm_kernel<> # kernel launch
out @ out_gate # kernel launch
# ... etc
```

We have elementwise operations between that will launch their own kernels, we need to get them in, however this means that we need to manage the chunking of output in Triton. We also have independent normalization kernels which cause their own kernel launches. Every time we launch a new kernel, we will need to write and read from memory. This is the main bottleneck of GPUs, so if we can get away with less kernel launches with work-groups that can perform meaningful work, we should get faster.

We can see that in PyTorch.compile, when running with logs, that the result is only 3 GPU kernels, 1 MM kernel for the first part, and 1 BMM kernel for the second part, and a final one for the out gate, implying a fuse of the norms and intermediaries.
![[Pasted image 20250817094326.png]]

---

We fuse the 2 first kernels norm and MM, small progress:
```python
norm_kernel<> #kernel launch
mm_kernel<> #kernel launch
```
‚ö° 1582 ¬µs üêå 1678 ¬µs       
‚ö° 9.71 ms üêå 13.2 ms

We then try to think of how to fuse in the element-wise operations in here as well. The tricky part is that left and left-gate are computed by the same MM, and currently they are just stacked together, being computed at the same time by different thread groups. What we could try here is that we need to get chunks of left + lg and right + rg into the same accumulator on the same thread group. This means that we need to rethink our data layout, and swizzle together the weights of the block with their gates. This is a bit tricky.

Our first attempt here involves padding the data:
```python
def pack_w_interleaved(weights):
    WL = weights['left_proj.weight']
    WLG = weights['left_gate.weight']
    WR = weights['right_proj.weight']
    WRG = weights['right_gate.weight']
    WOG = weights['out_gate.weight']
    H, K = WL.shape

    # Stack to [5, H, K]
    ws = torch.stack([WL, WLG, WR, WRG, WOG], dim=0)

    # Permute to group by hidden dimension: [H, 5, K]
    ws = ws.permute(1, 0, 2)

    # Pad the 'role' dimension (dim=1) from 5 to 8 with zeros.
    # The format for F.pad is (pad_last_dim, pad_second_to_last_dim, ...)
    # We want to add 3 zeros to the end of the second dimension.
    ws = F.pad(ws, (0, 0, 0, 3), "constant", 0)
    # Shape is now [H, 8, K]

    # Flatten into the final memory layout.
    # The layout is now [L0,LG0,R0,RG0,OG0,P,P,P, L1,LG1,R1,RG1,OG1,P,P,P, ...]
    # where P is a zero-padded vector.
    ws = ws.contiguous().view(8 * H, K)

    # Transpose for the matmul. Final shape: [K, 8*H]
    W_swizzled = ws.t().contiguous().to(torch.float16)
    return W_swizzled
```

This will allow us to use size-8-multiples to reshape the accumulator, mask in/out both the left/left_gate and right/right_gate, and then do elementwise fusion of all the intermediates without ever writing the gates to memory.

However, this does not yield a massive improvement:
‚ö° 1431 ¬µs üêå 1552 ¬µs       
‚ö° 9.09 ms üêå 9.23 ms

I thought that this was because of our increased matmul size. We are adding 3 massive dimensions that add 60% compute that does nothing. After thinking, I thought that we need to rethink the weight packing again. In order to avoid padding, I figured that a better solution would be to stack ONLY the left/lg/right/rg into a tensor, as 4 is a size that will not require padding on GPU. We will then perform 2 loads instead of 1(left/right + out), but save the 60% useless compute bloat. We do this and get:

‚ö° 1255 ¬µs üêå 1361 ¬µs       
‚ö° 8.38 ms üêå 8.49 ms

Great progress. We then see that the original code requires some reshaping and permuting, and transpositions:
```python
    left = left.view(bs, s1, s2, h).permute(0,3,1,2)
    right = right.view(bs, s1, s2, h).permute(0,3,1,2)
    out_p = torch.matmul(left.to(torch.float16),   
    right.to(torch.float16).transpose(-1, -2))
```

Instead of doing this, we can construct the output buffers correctly from the get-go in Triton, and then use a more complex stride construction to directly write into the correct locations for the BMM. 
```python
fused_norm_MM_gate_kernel<>
out_p = torch.matmul(left, right)
rest<>...
```
This was actually a surprisingly large leap for the larger matrices that takes us down to:

‚ö° 1086 ¬µs üêå 1246 ¬µs       
‚ö° 4.31 ms üêå 4.49 ms

I assume that this is due to avoiding copying of large matrices that the shape operation ops often involve.
We can see that we are actually outperforming the best PyTorch kernel on the larger input on the H100.
We have now arrived at something that looks like this:

```python
norm_mm_elementwise_kernel<> #kernel launch
bmm_kernel<> # kernel launch
norm_kernel<> # kernel launch
out @ out_gate # kernel launch
```
Since BMM is different from MM and needs a different 3-D PID structure, we will try to see if a 2 or 3-kernel solution is the best here.

As an intermediary best solution, I break out the 3 kernel launches to their own function and do a pytorch.compile() on them, meaning that we have a Triton-PyTorch mix which also gives us a time to beat with the future Triton-only solution. This turns out to be:

‚ö° 878 ¬µs üêå 1218 ¬µs       
‚ö° 3.27 ms üêå 3.46 ms

For the BMM fusion we also have to question ourselves a little bit. We will do a pass over the entire BMM during the first calculation, so in theory, it would make sense to collect mean/std data while we do this, and write it to HBM for the next kernel. This would allow us to do elementwise mean/std directly in the next fused kernel without having to first make a pass over all data and collect the statistics before doing another pass over the data with the elementwise normalization. 
However, this would also mean that we would need to put more threads than necessary in every group for the BMM, and store chunks of the BMM as we go along the K-dimension to collect the stats instead of just summing individual blocks up and writing them to memory directly.

We start with making the solution with 3 kernels. 

‚ö° 865 ¬µs üêå 999 ¬µs       
‚ö° 3.50 ms üêå 3.83 ms

We note that with and under seq-len 512, we actually do better with the initial pytorch kernel. Indicating that for short sequences, we might introduce too much overhead with out first kernel.

As I dont want to spend more time on this because of other competitions and a paper to write, I will do final small optimizations for the various GPUs by setting various Triton-configs and various routings between the Triton and the PyTorch solution based on the input size for every GPU arch.

We set up a comprehensive set of various Triton configurations for the various GPUs, and get our final result: 

‚ö° 865 ¬µs üêå 999 ¬µs       
‚ö° 3.27 ms üêå 3.46 ms

Looking back at the initial kernel:

‚ö° 3.05 ms üêå 3.14 ms       
‚ö° 13.9 ms üêå 14.7 ms

We see that with some hours of work, we have managed to speed up the execution of the Trimul algorithm on GPGPU by 3-4x for any server GPU of your choice, great success!

---
This is of course not the end of the theoretical line. I believe that there is another improvement of similar magnitude to be collected using the specific GPU intrinsic like TMA, Tensor Cores, and more efficient memory mapping. In general, I believe that my optimization with computing the gate and projection in the same warp could be wiped by just a more efficient GEMM in lower-level languages. However, this also involves understanding all the specific architectures, which I dont currently do. Maybe next competition. Thanks for reading.

[Full code and final implementation can be found here](https://github.com/arseniivanov/trimul)
