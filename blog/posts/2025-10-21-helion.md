# Helion and its auto-generated Triton kernels

A group from the PyTorch team involved in torch.compile recently released [Helion](https://github.com/pytorch/helion) which is a tile-based Python DSL similar to Triton, but at the PyTorch level of abstraction. 

The philosophy of Helion lies in that the search space of Triton kernels is hard to cover manually, even with automated configs that already exist in Triton. This is because the existing automated Triton configs do not handle various hardware optimization features like memory swizzling, grid size specification, block pointer/TMA choice, persistent kernels or some advanced loop reordering specifics. Triton can only change hyper-parameters of a kernel, while Helion can change the kernel itself using reordering of loops and similar optimizations. A presentation and a rationale behind Helion by the creators can be found in [this presentation/talk](https://www.youtube.com/watch?v=MBOPzfl1JBo)

A necessary thing to note for this read is that Helion is really new, the 0.2 Beta release was on yesterday 20th October, so it should be given some time to mature before we make final-ish judgement.

Triton also, for good reason i believe, imposes quite strict requirements on the programmer. You can for example only define compile-time arrays that are multiples of 2, as this is a hardware-aligned size. You also have to specify compile-time constants for a lot of functions using constexpr. We will later on discuss why I believe this can cause problems with Helion's hands-off approach that will handle all these requirements for you.

Let's instead start with a hands-on example:
```python
@helion.kernel(static_shapes=True)
def helion_mm(x: torch.Tensor, y: torch.Tensor, bias: Optional[torch.Tensor] = None):
    m, k = x.size()
    _, n = y.size()
    
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)

    for tile_m, tile_n in hl.tile([m, n]):
        acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)

        for tile_k in hl.tile(k):
            acc = torch.addmm(acc, x[tile_m, tile_k], y[tile_k, tile_n])

        acc = acc.to(out.dtype)
        
        if bias is not None:
            bias_tile = bias[tile_n]
            acc = acc + bias_tile

        out[tile_m, tile_n] = acc

    return out
    
```
This GEMM example generates about 1200 Triton kernels, and iteratively prunes/evolves the various configuration values and generate new kernels that explore the optimization spaces of variations of the new best kernels. This optimization seems quite well-done, the kernel execution time decreases every step.

The whole process takes about 10 minutes on my 3080Ti, after which we get a nice config on the form:

```python
config_2048=helion.Config(block_sizes=[64, 64, 32], indexing='block_ptr', l2_groupings=[8], load_eviction_policies=['', 'first', 'first'], loop_orders=[[1, 0]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[])
```
We can run another optimization process with a larger size for K and we get:

```python
config_8192=helion.Config(block_sizes=[64, 64, 32], indexing='block_ptr', l2_groupings=[1], load_eviction_policies=['first', 'last', 'last'], loop_orders=[[1, 0]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 4], range_unroll_factors=[0, 1], range_warp_specializes=[])
```

Explanations for all of the different fields can be found in the [Configurations](https://github.com/pytorch/helion?tab=readme-ov-file) section on the Helion github.

After comparing compiled PyTorch, Triton and Helion, we get this result for the amount of TFLOPS (higher throughput is better):

```python
        K     Triton    PyTorch     Helion
0   512.0  39.088622  48.453301  38.401121
1  1024.0  41.203562  54.485096  43.354861
2  2048.0  42.626106  56.828462  42.802728
3  8192.0  41.386377  57.622134  41.947110
```

We can see that we dont really improve the Triton kernel, and the reason for the PyTorch domination is that there are closed-source, hand-crafted Cutlass/PTX kernels for these common matmul-sizes in PyTorch.

In order to make a more fair comparison, let's try a more complex kernel. For this, I decided to go with a simplified first fused part of the TriMul kernel I made for the competition last month. This part is a 2-pass LayerNorm mean/var calculation, followed by an MM with fused elementwise ops, followed by a PID-based branching for conditional fused calculations on the out-gate.

In this optimization search, we notice a lot of interesting things:
1) A lot of MLIR compilation errors for various kernels
2) A LOT of mismatched outputs for various kernels

This is kind of expected due to the nature of the problem. The reason for the separate mean/var passes in my initial Triton solution, found [here](https://github.com/arseniivanov/trimul/blob/main/triton_h100.py) was to keep the numerical stability. When only doing a single pass over K, I found that we do not match the reference. I can imagine that some unrolls or loop reorders will cause problems here.

However, its worth to note that Helion manages these issues quite gracefully, we simply skip over all of the broken/mismatched configs and the optimization runs fine.

Here is the comparison for our more complex problem, and [here is the gist for the code to reproduce it](https://gist.github.com/arseniivanov/4b872aca7a4a9d122ccb5fe5e593fdc1):

```python
Running Triton kernel...
  -> Result: 0.0258 ms,  6.52 TFLOPS

Running Helion kernel...
  -> Result: 0.0240 ms,  7.01 TFLOPS
```
(note that the config is for 3080Ti and you will need to retune it for your GPU)

We can see that the Helion solution is about half the size in rows of the Triton solution, mainly because of not needing to handle PIDs, and runs faster (8% average over 250 comparisons) for this more complex example, showcasing the improvement.

---
Normally, as you go up the chain of abstraction levels, you are expected to be able to write more general code with a loss of performance. In this case, we seem to get the promise both - higher abstraction AND better performance, how?
Paradoxically, I feel like Helion lands further up on the optimization/devtime-trade-off curve while being a "higher-level" language. 

![[Pasted image 20251017184540.png]]

I believe that Helion is not strictly a higher abstraction than Triton, despite the easier syntax, as it requires the programmer to know **more** about the task than Triton does. This is because Helion handles a lot for you in the background, and has long iteration windows. You also have to drop the "per-program" GPU programming abstraction and think of tiling the work from the view of the full problem.

For example, if you were to create an array/range of size 33, Helion will create an array of size of the nearest larger multiple of 2 due to Triton restrictions, so 64, and then use masking to specify the first 33 elements, then run the kernel optimization search. 

I believe that oftenmost, this is not what you want. At large array sizes, you might fill the caches with 0s, and end up with 10-or-larger minute optimizations that leads nowhere. 
### What I think Helion could use
Warnings for all the hard constraints that Triton needs. Every time padding is added or assumptions are made that would have to be explicit in Triton, this overhead should be communicated to the programmer before kernel optimization starts. This can avoid 10-minute debug iteration loops and would make Helion more approachable as a first-development choice.
### How I might use Helion
I actually see Helion as a post-Triton/Pytorch level kernel implementation. Once you have iterated to a good kernel with good runtime and sizes that compile nicely, you can then rewrite it in Helion in order to cover a larger search space you might have missed with the knowledge that you are optimizing for the right shapes.

Another interesting thing is that you can set:
`HELION_PRINT_OUTPUT_CODE=1` as an env-var or pass `print_output_code=True` into the decorator which returns you the the created Triton kernel. This can be useful if looking to explore or improve your Triton kernel with additions such as Gluon.

This feature also personally helped me to understand the various flags/function attributes in Triton better. In many cases, I am simply not aware, or fully understand when a flag to a function is applicable, or why we can evict the data from the cache, and being provided with a kernel that applies this and having a base kernel to compare to kind of improves this learning process.
### Concluding
At the same time, it becomes a question, at what point will inlining blocks in PTX, or using Triton extensions such as Gluon make more sense? Will Helion be able to catch up with efficient compilation and testing without causing a combinatorial explosion of kernels? It seems like the [Blackwell attention](https://github.com/pytorch/helion/blob/main/examples/blackwell_attention.py) for Helion kind of goes this way by providing inline PTX for certain functions.

I get vibes similar to a battle that was already "won" by Triton once - against Apache TVM's vast optimization space, given the utilization of the former over the latter in scientific ML-optimization papers. This makes me curiously doubtful, but I will follow Helion's journey with interest.
